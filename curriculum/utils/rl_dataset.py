# Copyright 2024 Bytedance Ltd. and/or its affiliates
# Updated by AgentTestable
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from collections import defaultdict
import os
import random
from typing import Any, Dict, List, Optional

import numpy as np
import torch
from datasets import load_dataset
from jinja2 import Template
from torch.utils.data import Dataset
from transformers import PreTrainedTokenizer, ProcessorMixin

from curriculum.utils import torch_functional as VF
from verl.utils.dataset.rl_dataset import RLHFDataset


def collate_fn(features: List[Dict[str, Any]]) -> Dict[str, Any]:
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)
    for feature in features:
        for key, value in feature.items():
            if isinstance(value, torch.Tensor):
                tensors[key].append(value)
            else:
                non_tensors[key].append(value)

    for key, value in tensors.items():
        tensors[key] = torch.stack(value, dim=0)

    for key, value in non_tensors.items():
        non_tensors[key] = np.array(value, dtype=object)

    return {**tensors, **non_tensors}


class AutoGeneratedRLHFDataset(RLHFDataset):
    """
    We assume the dataset contains a column that contains prompts and other information
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        processor: Optional[ProcessorMixin],
        prompt_key: str = "prompt",
        image_key: str = "images",
        max_prompt_length: int = 1024,
        truncation: str = "error",
        custom_format_prompt: Optional[str] = None,
        custom_fake_data_size: int = 1000,
        question_dir: Optional[str] = None,
    ):
        self.tokenizer = tokenizer
        self.processor = processor
        self.prompt_key = prompt_key
        self.image_key = image_key
        self.max_prompt_length = max_prompt_length
        self.truncation = truncation
        self.question_dir = question_dir

        # 2 customised settings
        # 2 customised settings
        self.custom_fake_data_size = custom_fake_data_size
        self.custom_format_prompt = custom_format_prompt
        self.custom_format_prompt_path = None
        self.custom_format_template = None
        self._last_prompt_mtime = 0
        self._current_loaded_prompt_path = None

        if custom_format_prompt:
            if os.path.exists(custom_format_prompt) and os.path.isfile(custom_format_prompt):
                self.custom_format_prompt_path = custom_format_prompt
                self._load_prompt_template()
            else:
                 # treat as string
                 self.custom_format_template = Template(custom_format_prompt)
                 self.custom_format_prompt = custom_format_prompt
        
        # Caching for good examples
        self._cached_good_examples = []
        self._last_files_state = None  # Will store (filenames, mtimes) tuple

    def _load_good_examples(self, top_n: int = 3) -> List[Dict[str, str]]:
        """Load top N questions with self_consistency_score == 0.5 from last 5 files."""
        questions_dir = self.question_dir
        
        # Check if directory exists
        if questions_dir is None or not os.path.exists(questions_dir):
            return []
        
        # Get all results_sc_*.json files
        try:
            all_files = [f for f in os.listdir(questions_dir) if f.startswith("results_sc_") and f.endswith(".json")]
            if not all_files:
                return []
            
            # Get full paths and mtimes
            file_paths = [os.path.join(questions_dir, f) for f in all_files]
            mtimes = [os.path.getmtime(p) for p in file_paths]
            
            # Pair them up and sort by mtime, newest first
            paired = sorted(zip(all_files, mtimes), key=lambda x: x[1], reverse=True)
            
            # Take only the last 5 files for the state check
            current_top_files = paired[:5]
            current_state = tuple(current_top_files)
            
            # Check cache
            if self._last_files_state == current_state:
                print(f"ðŸ˜Š Using cached good examples (state unchanged)")
                if len(self._cached_good_examples) > top_n:
                    return random.sample(self._cached_good_examples, top_n)
                return self._cached_good_examples
            
            # State changed, reload
            good_examples = []
            for file, _ in current_top_files:
                file_path = os.path.join(questions_dir, file)
                try:
                    import json
                    with open(file_path, 'r') as f:
                        data = json.load(f)
                        for item in data:
                            sc_score = item.get("self_consistency_score", 0.0)
                            # Check if sc_score is exactly 0.5
                            if sc_score == 0.5:
                                question = item.get("question", "")
                                predict_answer = item.get("predict_answer", "")
                                if question and predict_answer:
                                    good_examples.append({
                                        "question": question,
                                        "answer": predict_answer
                                    })
                except Exception as e:
                    # Skip files that can't be read
                    continue
            
            print(f"ðŸ˜ŠðŸ˜ŠðŸ˜Š Reloaded {len(good_examples)} good examples from changed files")
            # Update cache
            self._cached_good_examples = good_examples
            self._last_files_state = current_state
            
            if len(good_examples) > top_n:
                return random.sample(good_examples, top_n)
            return good_examples
            
        except Exception as e:
            print(f"âš ï¸ Error loading good examples: {e}")
            return []

    def _load_prompt_template(self):
        # If we have a configured path, try to find the "latest" version of it
        # We assume the user configures ".../prompt_0.jinja" and we look for ".../prompt_{N}.jinja"
        target_path = self.custom_format_prompt_path
        
        if target_path and os.path.isdir(os.path.dirname(target_path)):
            dirname = os.path.dirname(target_path)
            basename = os.path.basename(target_path)
            
            # extract prefix "prompt_" and extension ".jinja" from "prompt_0.jinja"
            # simple assumption: format is prefix_N.ext
            import re
            match = re.match(r"(.*)_(\d+)(\..+)$", basename)
            if match:
                prefix = match.group(1) # e.g. "prompt"
                # ext = match.group(3)    # e.g. ".jinja"
                
                # Scan directory for all matching files
                candidates = []
                try:
                    for fname in os.listdir(dirname):
                        # match prompt_N.jinja
                        m = re.match(rf"{re.escape(prefix)}_(\d+)(\..+)$", fname)
                        if m:
                            n = int(m.group(1))
                            candidates.append((n, os.path.join(dirname, fname)))
                except Exception:
                    pass
                
                if candidates:
                    # Sort by N descending
                    candidates.sort(key=lambda x: x[0], reverse=True)
                    latest_n, latest_path = candidates[0]
                    target_path = latest_path
                    # print(f"DEBUG: Found latest prompt file: {target_path} (N={latest_n})")

        if target_path:
             try:
                 # Check mtime of the target path
                 if not os.path.exists(target_path):
                     return

                 mtime = os.path.getmtime(target_path)
                 # Reload if file changed OR if we switched to a new file (path changed)
                 if mtime > self._last_prompt_mtime or target_path != self.custom_format_prompt_path:
                     with open(target_path, 'r', encoding='utf-8') as f:
                         content = f.read()
                     self.custom_format_template = Template(content)
                     self.custom_format_prompt = content
                     self._last_prompt_mtime = mtime
                     
                     # Update the path so we know we are currently on this file
                     # (Use a separate variable to track the *current* loaded path vs the *initial* config path)
                     self._current_loaded_prompt_path = target_path
                     print(f"ðŸ˜Š Reloaded prompt template from {target_path}")
             except Exception as e:
                 print(f"âš ï¸ Error loading prompt template: {e}")

    def _build_messages(self) -> List[Dict[str, Any]]:
        # Reload prompt if file changed
        self._load_prompt_template()

        if self.custom_format_template:
            # Load good examples dynamically
            good_examples = self._load_good_examples(top_n=3)
            
            # Build examples section
            examples_text = ""
            if good_examples:
                examples_text = "\n\n### EXAMPLES OF PREVIOUS GOOD PROBLEMS AND BOXED ANSWERS:\n"
                for i, example in enumerate(good_examples, 1):
                    examples_text += f"Example {i}:\n"
                    examples_text += f"<question>\n{example['question'].strip()}\n</question>\n\n"
                    examples_text += f"\\boxed{{{example['answer'].strip()}}}\n\n"
            
            # Render the template
            system_content = self.custom_format_template.render(examples_text=examples_text)

            user_content = (
                "Generate one brand-new, challenging competition-level math problem in English now. "
                "Ensure it is wrapped in <question> tags and followed by a \\boxed{answer}. "
                "Do NOT include any preamble, conversational filler, or scratchpad."
            )
            
            return [
                {"role": "system", "content": system_content},
                {"role": "user", "content": user_content}
            ]
        
        # Fallback if no template (should generally not happen if correctly configured)
        return [{"role": "user", "content": "Generate a challenging math reasoning question."}]

        
    def __len__(self):
        return self.custom_fake_data_size

    def __getitem__(self, index):
        example: dict = {}
        messages = self._build_messages()

        if self.tokenizer.chat_template:
            prompt = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        else:
            prompt = "system: " + messages[0]["content"] + '\n' + "user: " + messages[1]["content"]
        
        model_inputs = self.tokenizer([prompt], add_special_tokens=False, return_tensors="pt")
        input_ids = model_inputs.pop("input_ids")[0]
        attention_mask = model_inputs.pop("attention_mask")[0]
        position_ids = torch.clip(attention_mask.cumsum(dim=0) - 1, min=0, max=None)  # (seq_length,)

        input_ids, attention_mask, position_ids = VF.postprocess_data(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            max_length=self.max_prompt_length,
            pad_token_id=self.tokenizer.pad_token_id,
            left_pad=True,
            truncation=self.truncation,
        )
        raw_prompt_ids = self.tokenizer.encode(prompt, add_special_tokens=False)
        if len(raw_prompt_ids) > self.max_prompt_length:
            if self.truncation == "left":
                raw_prompt_ids = raw_prompt_ids[-self.max_prompt_length :]
            elif self.truncation == "right":
                raw_prompt_ids = raw_prompt_ids[: self.max_prompt_length]
            elif self.truncation == "error":
                raise RuntimeError(f"Prompt length {len(raw_prompt_ids)} is longer than {self.max_prompt_length}.")

        # TODO(wuxibin): We still need a dummy tensor to make sure DataProto.batch is not empty.
        # Remove this after deprecate DataProto by TensorDict.
        example["dummy_tensor"] = torch.tensor([0], dtype=torch.uint8)
        # example["input_ids"] = input_ids
        #example["attention_mask"] = attention_mask
        #example["postion_ids"] = position_ids
        #example["raw_prompt_ids"] = raw_prompt_ids
        # example["ground_truth"] = ""
        example["raw_prompt"] = messages
        example["index"] = index
        example["tools_kwargs"] = {}
        example["interaction_kwargs"] = {}
        return example
