# Copyright 2024 Bytedance Ltd. and/or its affiliates
# Updated by AgentTestable
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from collections import defaultdict
import os
import random
from typing import Any, Dict, List, Optional

import numpy as np
import torch
from datasets import load_dataset
from jinja2 import Template
from torch.utils.data import Dataset
from transformers import PreTrainedTokenizer, ProcessorMixin

from curriculum.utils import torch_functional as VF
from verl.utils.dataset.rl_dataset import RLHFDataset


def collate_fn(features: List[Dict[str, Any]]) -> Dict[str, Any]:
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)
    for feature in features:
        for key, value in feature.items():
            if isinstance(value, torch.Tensor):
                tensors[key].append(value)
            else:
                non_tensors[key].append(value)

    for key, value in tensors.items():
        tensors[key] = torch.stack(value, dim=0)

    for key, value in non_tensors.items():
        non_tensors[key] = np.array(value, dtype=object)

    return {**tensors, **non_tensors}


class AutoGeneratedRLHFDataset(RLHFDataset):
    """
    We assume the dataset contains a column that contains prompts and other information
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        processor: Optional[ProcessorMixin],
        prompt_key: str = "prompt",
        image_key: str = "images",
        max_prompt_length: int = 1024,
        truncation: str = "error",
        custom_format_prompt: Optional[str] = None,
        custom_fake_data_size: int = 1000,
        question_dir: Optional[str] = None,
    ):
        self.tokenizer = tokenizer
        self.processor = processor
        self.prompt_key = prompt_key
        self.image_key = image_key
        self.max_prompt_length = max_prompt_length
        self.truncation = truncation
        self.question_dir = question_dir

        # 2 customised settings
        self.custom_format_prompt = custom_format_prompt
        self.custom_fake_data_size = custom_fake_data_size
        self.custom_format_prompt = None
        if custom_format_prompt:
            with open(custom_format_prompt, encoding="utf-8") as f:
                self.custom_format_prompt = f.read()
        
        # Caching for good examples
        self._cached_good_examples = []
        self._last_files_state = None  # Will store (filenames, mtimes) tuple

    def _load_good_examples(self, top_n: int = 3) -> List[Dict[str, str]]:
        """Load top N questions with self_consistency_score == 0.5 from last 5 files."""
        questions_dir = self.question_dir
        
        # Check if directory exists
        if questions_dir is None or not os.path.exists(questions_dir):
            return []
        
        # Get all results_sc_*.json files
        try:
            all_files = [f for f in os.listdir(questions_dir) if f.startswith("results_sc_") and f.endswith(".json")]
            if not all_files:
                return []
            
            # Get full paths and mtimes
            file_paths = [os.path.join(questions_dir, f) for f in all_files]
            mtimes = [os.path.getmtime(p) for p in file_paths]
            
            # Pair them up and sort by mtime, newest first
            paired = sorted(zip(all_files, mtimes), key=lambda x: x[1], reverse=True)
            
            # Take only the last 5 files for the state check
            current_top_files = paired[:5]
            current_state = tuple(current_top_files)
            
            # Check cache
            if self._last_files_state == current_state:
                print(f"ðŸ˜Š Using cached good examples (state unchanged)")
                if len(self._cached_good_examples) > top_n:
                    return random.sample(self._cached_good_examples, top_n)
                return self._cached_good_examples
            
            # State changed, reload
            good_examples = []
            for file, _ in current_top_files:
                file_path = os.path.join(questions_dir, file)
                try:
                    import json
                    with open(file_path, 'r') as f:
                        data = json.load(f)
                        for item in data:
                            sc_score = item.get("self_consistency_score", 0.0)
                            # Check if sc_score is exactly 0.5
                            if sc_score == 0.5:
                                question = item.get("question", "")
                                predict_answer = item.get("predict_answer", "")
                                if question and predict_answer:
                                    good_examples.append({
                                        "question": question,
                                        "answer": predict_answer
                                    })
                except Exception as e:
                    # Skip files that can't be read
                    continue
            
            print(f"ðŸ˜ŠðŸ˜ŠðŸ˜Š Reloaded {len(good_examples)} good examples from changed files")
            # Update cache
            self._cached_good_examples = good_examples
            self._last_files_state = current_state
            
            if len(good_examples) > top_n:
                return random.sample(good_examples, top_n)
            return good_examples
            
        except Exception as e:
            print(f"âš ï¸ Error loading good examples: {e}")
            return []

    def _build_messages(self) -> List[Dict[str, Any]]:
        if self.custom_format_prompt and "questioner_format" in self.custom_format_prompt:
            # Load good examples dynamically
            good_examples = self._load_good_examples(top_n=3)
            
            # Build examples section
            examples_text = ""
            if good_examples:
                examples_text = "\n\n### EXAMPLES OF PREVIOUS GOOD PROBLEMS AND BOXED ANSWERS:\n"
                for i, example in enumerate(good_examples, 1):
                    examples_text += f"Example {i}:\n"
                    examples_text += f"<question>\n{example['question'].strip()}\n</question>\n\n"
                    examples_text += f"\\boxed{{{example['answer'].strip()}}}\n\n"
            
            system_content = (
                "You are an expert competition-math problem setter. Your mission is to create original, high-level math competition problems for advanced students (AIME/AMC/IMO level).\n\n"
                "### COMPULSORY RULES (Violations will be penalized):\n"
                "1. **Language**: The entire output MUST be in English. No other languages are permitted.\n"
                "2. **Format**: You MUST use this EXACT structure with no extra text:\n\n"
                "<question>\n"
                "{Complete problem statement in English}\n"
                "</question>\n\n"
                "\\boxed{final_numerical_or_simplified_answer}\n\n"
                "3. **Strictly No Reasoning**: Do NOT include any 'scratchpad', 'thoughts', or explanations. Output ONLY the tags and the boxed answer.\n"
                "4. **Originality**: The problem must be a new creation. Do not copy famous or existing problems.\n\n"
                "### FORMAT EXAMPLE:\n"
                "<question>\n"
                "What is the sum of the roots of the equation x^2 - 5x + 6 = 0?\n"
                "</question>\n\n"
                "\\boxed{5}"
                + examples_text
            )
            
            user_content = (
                "Generate one brand-new, challenging competition-level math problem in English now. "
                "Ensure it is wrapped in <question> tags and followed by a \\boxed{answer}. "
                "Do NOT include any preamble, conversational filler, or scratchpad."
            )
            
            return [
                {"role": "system", "content": system_content},
                {"role": "user", "content": user_content}
            ]

        raise RuntimeError("Custom format prompt not found")
        
        # Fallback for other custom formats
        if self.custom_format_prompt:
            # If custom_format_prompt is a file path, it's already loaded into self.custom_format_prompt string
            # If it's directly a string, it's also fine.
            return [{"role": "user", "content": self.custom_format_prompt}]
        
        return [{"role": "user", "content": "Generate a challenging math reasoning question."}]

        
    def __len__(self):
        return self.custom_fake_data_size

    def __getitem__(self, index):
        example: dict = {}
        messages = self._build_messages()

        if self.tokenizer.chat_template:
            prompt = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        else:
            prompt = "system: " + messages[0]["content"] + '\n' + "user: " + messages[1]["content"]
        
        model_inputs = self.tokenizer([prompt], add_special_tokens=False, return_tensors="pt")
        input_ids = model_inputs.pop("input_ids")[0]
        attention_mask = model_inputs.pop("attention_mask")[0]
        position_ids = torch.clip(attention_mask.cumsum(dim=0) - 1, min=0, max=None)  # (seq_length,)

        input_ids, attention_mask, position_ids = VF.postprocess_data(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            max_length=self.max_prompt_length,
            pad_token_id=self.tokenizer.pad_token_id,
            left_pad=True,
            truncation=self.truncation,
        )
        raw_prompt_ids = self.tokenizer.encode(prompt, add_special_tokens=False)
        if len(raw_prompt_ids) > self.max_prompt_length:
            if self.truncation == "left":
                raw_prompt_ids = raw_prompt_ids[-self.max_prompt_length :]
            elif self.truncation == "right":
                raw_prompt_ids = raw_prompt_ids[: self.max_prompt_length]
            elif self.truncation == "error":
                raise RuntimeError(f"Prompt length {len(raw_prompt_ids)} is longer than {self.max_prompt_length}.")

        # TODO(wuxibin): We still need a dummy tensor to make sure DataProto.batch is not empty.
        # Remove this after deprecate DataProto by TensorDict.
        example["dummy_tensor"] = torch.tensor([0], dtype=torch.uint8)
        # example["input_ids"] = input_ids
        #example["attention_mask"] = attention_mask
        #example["postion_ids"] = position_ids
        #example["raw_prompt_ids"] = raw_prompt_ids
        # example["ground_truth"] = ""
        example["raw_prompt"] = messages
        example["index"] = index
        example["tools_kwargs"] = {}
        example["interaction_kwargs"] = {}
        return example
