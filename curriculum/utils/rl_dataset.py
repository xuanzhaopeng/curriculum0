# Copyright 2024 Bytedance Ltd. and/or its affiliates
# Updated by AgentTestable
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from collections import defaultdict
import os
from typing import Any, Dict, List, Optional

import numpy as np
import torch
from datasets import load_dataset
from jinja2 import Template
from torch.utils.data import Dataset
from transformers import PreTrainedTokenizer, ProcessorMixin

from curriculum.utils import torch_functional as VF
from verl.utils.dataset.rl_dataset import RLHFDataset


def collate_fn(features: List[Dict[str, Any]]) -> Dict[str, Any]:
    tensors = defaultdict(list)
    non_tensors = defaultdict(list)
    for feature in features:
        for key, value in feature.items():
            if isinstance(value, torch.Tensor):
                tensors[key].append(value)
            else:
                non_tensors[key].append(value)

    for key, value in tensors.items():
        tensors[key] = torch.stack(value, dim=0)

    for key, value in non_tensors.items():
        non_tensors[key] = np.array(value, dtype=object)

    return {**tensors, **non_tensors}


class AutoGeneratedRLHFDataset(RLHFDataset):
    """
    We assume the dataset contains a column that contains prompts and other information
    """

    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        processor: Optional[ProcessorMixin],
        prompt_key: str = "prompt",
        image_key: str = "images",
        max_prompt_length: int = 1024,
        truncation: str = "error",
        custom_format_prompt: Optional[str] = None,
        custom_fake_data_size: int = 1000,
        question_dir: Optional[str] = None,
    ):
        self.tokenizer = tokenizer
        self.processor = processor
        self.prompt_key = prompt_key
        self.image_key = image_key
        self.max_prompt_length = max_prompt_length
        self.truncation = truncation
        self.question_dir = question_dir

        # 2 customised settings
        self.custom_format_prompt = custom_format_prompt
        self.custom_fake_data_size = custom_fake_data_size
        self.custom_format_prompt = None
        if custom_format_prompt:
            with open(custom_format_prompt, encoding="utf-8") as f:
                self.custom_format_prompt = f.read()
        
        # Caching for good examples
        self._cached_good_examples = []
        self._last_files_state = None  # Will store (filenames, mtimes) tuple

    def _load_good_examples(self, top_n: int = 3) -> List[Dict[str, str]]:
        """Load top N questions with self_consistency_score == 0.5 from last 5 files."""
        questions_dir = self.question_dir
        
        # Check if directory exists
        if questions_dir is None or not os.path.exists(questions_dir):
            return []
        
        # Get all results_sc_*.json files
        try:
            all_files = [f for f in os.listdir(questions_dir) if f.startswith("results_sc_") and f.endswith(".json")]
            if not all_files:
                return []
            
            # Get full paths and mtimes
            file_paths = [os.path.join(questions_dir, f) for f in all_files]
            mtimes = [os.path.getmtime(p) for p in file_paths]
            
            # Pair them up and sort by mtime, newest first
            paired = sorted(zip(all_files, mtimes), key=lambda x: x[1], reverse=True)
            
            # Take only the last 5 files for the state check
            current_top_files = paired[:5]
            current_state = tuple(current_top_files)
            
            # Check cache
            if self._last_files_state == current_state:
                print(f"üòäüòäüòä Using cached good examples (state unchanged)")
                return self._cached_good_examples[:top_n]
            
            # State changed, reload
            good_examples = []
            for file, _ in current_top_files:
                file_path = os.path.join(questions_dir, file)
                try:
                    import json
                    with open(file_path, 'r') as f:
                        data = json.load(f)
                        for item in data:
                            sc_score = item.get("self_consistency_score", 0.0)
                            # Check if sc_score is exactly 0.5
                            if sc_score == 0.5:
                                question = item.get("question", "")
                                predict_answer = item.get("predict_answer", "")
                                if question and predict_answer:
                                    good_examples.append({
                                        "question": question,
                                        "answer": predict_answer
                                    })
                except Exception as e:
                    # Skip files that can't be read
                    continue
            
            print(f"üòäüòäüòä Reloaded {len(good_examples)} good examples from changed files")
            # Update cache
            self._cached_good_examples = good_examples
            self._last_files_state = current_state
            
            return good_examples[:top_n]
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading good examples: {e}")
            return []

    def _build_messages(self) -> List[Dict[str, Any]]:
        if "questioner_format" in self.custom_format_prompt:
            # Load good examples dynamically
            good_examples = self._load_good_examples(top_n=3)
            
            # Build examples section
            examples_text = ""
            if good_examples:
                examples_text = "\n\nHere are some good examples of questions with appropriate difficulty:\n\n"
                for i, example in enumerate(good_examples, 1):
                    examples_text += f"Example {i}:\n"
                    examples_text += f"<question>\n{example['question']}\n</question>\n\n"
                    examples_text += f"\\boxed{{{example['answer']}}}\n\n"
            
            # print('detected questioner_format')
            return [
                {
                    "role": "system",
                    "content": (
                        "You are an expert competition-math problem setter.\n"
                        "FIRST, in your private scratch-pad, think step-by-step to design a brand-new, non-trivial problem. "
                        "The problem must be in English, and could come from any field of mathematics, including but not limited to algebra, geometry, number theory, combinatorics, prealgebra, probability, statistics, and calculus. "
                        "Aim for a difficulty such that fewer than 30 % of advanced high-school students could solve it. "
                        "Avoid re-using textbook clich√©s or famous contest problems.\n"
                        "THEN, without revealing any of your private thoughts, output **exactly** the following two blocks:\n\n"
                        "<question>\n"
                        "{The full problem statement in English on one or more lines}\n"
                        "</question>\n\n"
                        r"\boxed{final_answer}"
                        "\n\n"
                        "Do NOT output anything else‚Äîno explanations, no extra markup."
                        + examples_text
                    )
                },
                {
                    "role": "user",
                    "content": (
                        "Generate one new, challenging reasoning question now. "
                        "Remember to format the output exactly as instructed."
                    )
                }
            ]
        if self.custom_format_prompt:
            format_prompt = Template(self.custom_format_prompt.strip())
            prompt_str = format_prompt.render(content=prompt_str)
        
        return [{"role": "user", "content": prompt_str}]

        
    def __len__(self):
        return self.custom_fake_data_size

    def __getitem__(self, index):
        example: dict = {}
        messages = self._build_messages()

        if self.tokenizer.chat_template:
            prompt = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        else:
            prompt = "system: " + messages[0]["content"] + '\n' + "user: " + messages[1]["content"]
        
        model_inputs = self.tokenizer([prompt], add_special_tokens=False, return_tensors="pt")
        input_ids = model_inputs.pop("input_ids")[0]
        attention_mask = model_inputs.pop("attention_mask")[0]
        position_ids = torch.clip(attention_mask.cumsum(dim=0) - 1, min=0, max=None)  # (seq_length,)

        input_ids, attention_mask, position_ids = VF.postprocess_data(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            max_length=self.max_prompt_length,
            pad_token_id=self.tokenizer.pad_token_id,
            left_pad=True,
            truncation=self.truncation,
        )
        raw_prompt_ids = self.tokenizer.encode(prompt, add_special_tokens=False)
        if len(raw_prompt_ids) > self.max_prompt_length:
            if self.truncation == "left":
                raw_prompt_ids = raw_prompt_ids[-self.max_prompt_length :]
            elif self.truncation == "right":
                raw_prompt_ids = raw_prompt_ids[: self.max_prompt_length]
            elif self.truncation == "error":
                raise RuntimeError(f"Prompt length {len(raw_prompt_ids)} is longer than {self.max_prompt_length}.")

        # TODO(wuxibin): We still need a dummy tensor to make sure DataProto.batch is not empty.
        # Remove this after deprecate DataProto by TensorDict.
        example["dummy_tensor"] = torch.tensor([0], dtype=torch.uint8)
        # example["input_ids"] = input_ids
        #example["attention_mask"] = attention_mask
        #example["postion_ids"] = position_ids
        #example["raw_prompt_ids"] = raw_prompt_ids
        # example["ground_truth"] = ""
        example["raw_prompt"] = messages
        example["index"] = index
        example["tools_kwargs"] = {}
        example["interaction_kwargs"] = {}
        return example
